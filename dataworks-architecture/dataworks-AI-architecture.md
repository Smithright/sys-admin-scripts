# Smithright DataWorks - AI Architecture & Opportunities



DRAFT AI TECH STACK:
thousandeyes
    enterprise agent
langchain
db-gpt
        semantic kernel
fabric

GCP
    dialogflow

MLops

    langchain > enterprise kernel agent + guardian agents
        memgpt

        crewai
            mistral / codestral
        dialogflow
    vectorshift / flowise

    agent framework?
    agentic teaming framework?
    natural language code base querying and explanations

---

RESEARCH AREAS:

embeddings
RAG


about human-aware AI
longnet

mem-gpt
MemoryBank: Enhancing Large Language Models with Long-Term Memory
Larimar: Large Language Models with Episodic Memory Control
Summary: Proposes a brain-inspired architecture for LLMs that includes episodic memory control, enabling dynamic one-shot updates of knowledge without expensive re-training.
4. Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory
Summary: Proposes a framework that enhances LLMs with the ability to recall relevant thoughts before generating responses and update memory after generating responses, improving long-term conversation capabilities.
5. From LLM to Conversational Agent: A Memory Enhanced Architecture
Summary: Introduces RAISE, an architecture that incorporates a dual-component memory system, enhancing LLMs to maintain context and continuity in conversations.
8. Continual Learning for Large Language Models: A Survey
Summary: Reviews techniques for continual learning in LLMs, addressing the challenge of updating models with new information without extensive re-training.
17. Neural Data Management Techniques
Summary: Proposes innovative techniques for managing neuron data in LLMs, such as sliding window approaches and low-rank predictions.
Relevance: These techniques can improve the efficiency and effectiveness of our AI models, particularly in handling large-scale neural data​ (ar5iv)​.